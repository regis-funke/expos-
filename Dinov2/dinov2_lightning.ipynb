{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning the DPT-DinoV2-Small-KITTI Model with the DPT-DinoV2-Small-NYU Dataset\n",
    "\n",
    "In this Jupyter Notebook, we embark on a practical journey to familiarize ourselves with the intricacies of fine-tuning a pre-trained Dense Prediction Transformer (DPT) model, specifically the dpt-dinov2-small-kitti. Our objective is to adapt and fine-tune this model using the dpt-dinov2-small-nyu dataset, which presents a unique challenge in depth estimation tasks.\n",
    "\n",
    "This notebook is structured to provide a comprehensive workflow, starting from importing necessary libraries and defining key components such as custom datasets and models, to initializing and training the model with fine-tuned parameters. Along the way, we delve into each step with detailed explanations and code annotations, ensuring clarity and understanding of the processes involved.\n",
    "\n",
    "Our goal is to not only achieve effective fine-tuning of the DPT model but also to gain deeper insights into the model's architecture and the fine-tuning techniques. This exercise serves as a hands-on exploration into the realm of Transformer models and their application in depth estimation tasks, setting a foundation for further experimentation and research in this field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import time\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# Third-Party Library Imports\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "# PyTorch Imports\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# PyTorch Lightning Imports\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Transformers and Datasets Imports\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoImageProcessor, DPTForDepthEstimation, TrainingArguments\n",
    "\n",
    "from torchvision.transforms import Resize\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Keeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing current time to measure runtime\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPTLightningModule(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    A PyTorch Lightning module for depth estimation using the DPT (Dense Prediction Transformer) model.\n",
    "\n",
    "    This module is specifically configured for fine-tuning the DPT model on depth estimation tasks. It involves\n",
    "    freezing the entire model initially and then unfreezing the last two layers of the transformer along with the head\n",
    "    for training. The module uses Mean Squared Error as the loss function for depth estimation.\n",
    "\n",
    "    Methods:\n",
    "        forward: Performs a forward pass through the model.\n",
    "        common_step: A shared step used for both training and validation.\n",
    "        configure_optimizers: Sets up the optimizer for training.\n",
    "        training_step: Performs a training step.\n",
    "        validation_step: Performs a validation step.\n",
    "        train_dataloader: Loads the training dataset.\n",
    "        val_dataloader: Loads the validation dataset.\n",
    "        test_dataloader: Loads the test dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the DPTLightningModule, loads the pre-trained DPT model and sets up layer freezing.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = DPTForDepthEstimation.from_pretrained(\"facebook/dpt-dinov2-small-kitti\")\n",
    "\n",
    "        # Freeze all parameters of the model initially\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze the last two transformer layers\n",
    "        for layer in self.model.backbone.encoder.layer[-2:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        # Unfreeze the head of the model (specific classifier or regression layer)\n",
    "        for param in self.model.head.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Predicted depth map.\n",
    "        \"\"\"\n",
    "        return self.model(x).predicted_depth\n",
    "\n",
    "    def common_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        A shared step for calculating loss, used in both training and validation steps.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): A batch from the dataset.\n",
    "            batch_idx (int): The index of the batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Computed loss for the batch.\n",
    "        \"\"\"\n",
    "        pixel_values = batch['pixel_values']\n",
    "        labels = batch['labels']\n",
    "        preds = self(pixel_values)\n",
    "        loss = torch.nn.functional.mse_loss(preds, labels)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configures the optimizer for training.\n",
    "\n",
    "        Returns:\n",
    "            torch.optim.Optimizer: The Adam optimizer.\n",
    "        \"\"\"\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Performs a training step.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): A batch from the dataset.\n",
    "            batch_idx (int): The index of the batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The loss for the training step.\n",
    "        \"\"\"\n",
    "        loss = self.common_step(batch, batch_idx)\n",
    "        self.log(\"training_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Performs a validation step.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): A batch from the dataset.\n",
    "            batch_idx (int): The index of the batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The loss for the validation step.\n",
    "        \"\"\"\n",
    "        loss = self.common_step(batch, batch_idx)\n",
    "        self.log(\"validation_loss\", loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    # Note: Ensure that train_dataloader, val_dataloader, and test_dataloader are defined elsewhere in your code.\n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return val_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return test_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the NYU Depth Dataset (assuming it's already downloaded and stored locally)\n",
    "nyu_dataset = load_from_disk(\"C:/Downloads/nyu_depth_v2.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ispecting image dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First image dimensions: (640, 480)\n",
      "Unique image dimensions in the first 100 images: {(640, 480)}\n"
     ]
    }
   ],
   "source": [
    "# Access the first image in the dataset to check its dimensions\n",
    "first_image = nyu_dataset['train'][0]['image']\n",
    "\n",
    "# Convert to PIL Image if it's not already\n",
    "if not isinstance(first_image, Image.Image):\n",
    "    first_image = Image.fromarray(first_image)\n",
    "\n",
    "# Print the dimensions of the first image\n",
    "print(f\"First image dimensions: {first_image.size}\")  # Outputs (width, height)\n",
    "\n",
    "# Optionally, to get a more comprehensive view, you can check dimensions of multiple images\n",
    "image_sizes = set()\n",
    "for i in range(min(len(nyu_dataset['train']), 100)):  # Check first 100 images\n",
    "    image = nyu_dataset['train'][i]['image']\n",
    "    if not isinstance(image, Image.Image):\n",
    "        image = Image.fromarray(image)\n",
    "    image_sizes.add(image.size)\n",
    "\n",
    "print(f\"Unique image dimensions in the first 100 images: {image_sizes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating DPTLightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LightningModule\n",
    "model = DPTLightningModule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring CustomNYUDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNYUDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for loading and processing the NYU Depth Dataset for depth estimation tasks.\n",
    "\n",
    "    This dataset class is tailored to preprocess images and corresponding depth maps for training\n",
    "    a depth estimation model. It includes functionality to resize depth maps to match the output size of the model.\n",
    "\n",
    "    Attributes:\n",
    "        dataset (Dataset): The original NYU Depth Dataset.\n",
    "        processor (AutoImageProcessor): The image processor to preprocess the images.\n",
    "        output_size (tuple): The target output size (height, width) for resizing depth maps.\n",
    "\n",
    "    Methods:\n",
    "        __len__: Returns the size of the dataset.\n",
    "        __getitem__: Retrieves and preprocesses an item (image and depth map) from the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, processor, output_size):\n",
    "        \"\"\"\n",
    "        Initializes the CustomNYUDataset with the given dataset, processor, and output size.\n",
    "\n",
    "        Args:\n",
    "            dataset (Dataset): The original NYU Depth Dataset.\n",
    "            processor (AutoImageProcessor): The processor to preprocess the images.\n",
    "            output_size (tuple): The target output size (height, width) for resizing depth maps.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the size of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of items in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves and preprocesses an item from the dataset.\n",
    "\n",
    "        The method processes both the image and its corresponding depth map. The image is processed using\n",
    "        the provided processor, and the depth map is resized to match the model's output size.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the item to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with processed pixel values ('pixel_values') and labels ('labels').\n",
    "        \"\"\"\n",
    "        # Retrieve the item at the specified index\n",
    "        item = self.dataset[idx]\n",
    "\n",
    "        # Convert image and depth map to PIL Image if they are not already\n",
    "        image = Image.fromarray(item['image']) if not isinstance(item['image'], Image.Image) else item['image']\n",
    "        depth_map = Image.fromarray(item['depth_map']) if not isinstance(item['depth_map'], Image.Image) else item['depth_map']\n",
    "\n",
    "        # Process the image using the provided processor\n",
    "        processed_images = self.processor(images=[image], return_tensors=\"pt\")\n",
    "        input_tensor = processed_images[\"pixel_values\"].squeeze()\n",
    "\n",
    "        # Resize depth map to match model's output size\n",
    "        depth_map_resized = Resize((576, 736))(depth_map)  # Resize to (width, height)\n",
    "        depth_map_tensor = torch.tensor(np.array(depth_map_resized), dtype=torch.float32)\n",
    "\n",
    "        return {\"pixel_values\": input_tensor, \"labels\": depth_map_tensor}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Image Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Processor Initialization\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/dpt-dinov2-small-nyu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assertaining Model Output Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output Size: torch.Size([544, 736])\n"
     ]
    }
   ],
   "source": [
    "# Creating a dummy input tensor for the model with dimensions [batch_size, channels, height, width]\n",
    "# Here, batch_size=1, channels=3 (for RGB images), and the height and width are set to 480 and 640 respectively\n",
    "dummy_input = torch.randn(1, 3, 480, 640)\n",
    "\n",
    "# Using torch.no_grad() to disable gradient calculations, as we only need a forward pass\n",
    "# This reduces memory usage and speeds up the computation\n",
    "with torch.no_grad():\n",
    "    # Forward pass: Run the dummy input through the model to get the output\n",
    "    model_output = model(dummy_input)\n",
    "\n",
    "# Extracting the output size of the model, which is the spatial dimension of the output (height and width)\n",
    "# The shape of model_output is expected to be [batch_size, channels, height, width]\n",
    "# We take the last two values for height and width\n",
    "model_output_size = model_output.shape[-2:]\n",
    "\n",
    "# Printing the model's output size to verify its dimensions\n",
    "print(f\"Model Output Size: {model_output_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with the actual model output size\n",
    "# This is the size to which the depth maps will be resized\n",
    "# The output size is specified as (width, height)\n",
    "model_output_size = (544, 736)\n",
    "\n",
    "# Create an instance of the CustomNYUDataset for the training data\n",
    "# This instance will use the training portion of the nyu_dataset\n",
    "# It will process images using the specified 'processor' and resize depth maps to 'model_output_size'\n",
    "train_dataset = CustomNYUDataset(nyu_dataset['train'], processor, model_output_size)\n",
    "\n",
    "# Similarly, create an instance of the CustomNYUDataset for the validation data\n",
    "# This instance will use the validation portion of the nyu_dataset\n",
    "# Images and depth maps in the validation dataset are processed in the same way as the training dataset\n",
    "val_dataset = CustomNYUDataset(nyu_dataset['validation'], processor, model_output_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring the batch size for training and evaluation\n",
    "# A smaller batch size of 4 is chosen, which is a conservative size that helps prevent memory issues\n",
    "batch_size = 2\n",
    "train_batch_size = batch_size  # Setting the training batch size\n",
    "eval_batch_size = batch_size   # Setting the evaluation (validation) batch size\n",
    "\n",
    "# Defining a custom collate function for the DataLoader\n",
    "# This function prepares batches by stacking the individual items' pixel values and labels\n",
    "def collate_fn(batch):\n",
    "    # Extracting pixel values (input features) from each item in the batch\n",
    "    pixel_values = [item['pixel_values'] for item in batch]\n",
    "\n",
    "    # Extracting labels (target depth maps) from each item in the batch\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    # Stacking all pixel values and labels in the batch to create batch tensors\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    # Returning a dictionary with keys 'pixel_values' and 'labels' for the batch\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "# Creating DataLoaders for training and validation\n",
    "# DataLoaders are used to load the data in batches during training and validation\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=collate_fn, batch_size=train_batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=eval_batch_size)\n",
    "\n",
    "# Placeholder for creating a DataLoader for testing (if needed in the future)\n",
    "# Currently, testing DataLoader isn't implemented\n",
    "# test_dataloader = DataLoader(test_ds, collate_fn=collate_fn, batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting a Sample Batch from the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of ground truth depth maps: torch.Size([2, 576, 736])\n"
     ]
    }
   ],
   "source": [
    "# Fetching a single batch of data from the train DataLoader\n",
    "# This is typically done to inspect or debug the shape and content of the batch data\n",
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "# Extracting the labels (i.e., ground truth depth maps) from the fetched batch\n",
    "# 'labels' contains the depth maps that the model is supposed to predict\n",
    "labels = batch['labels']\n",
    "\n",
    "# Printing the size of the labels tensor\n",
    "# The size will show us the dimensions of the depth maps, including the batch size\n",
    "# This helps to confirm that the depth maps are correctly loaded and batched\n",
    "print(\"Size of ground truth depth maps:\", labels.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Trainer and Starting Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                  | Params\n",
      "------------------------------------------------\n",
      "0 | model | DPTForDepthEstimation | 37.2 M\n",
      "------------------------------------------------\n",
      "4.5 M     Trainable params\n",
      "32.7 M    Non-trainable params\n",
      "37.2 M    Total params\n",
      "148.790   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ece823e03b4009b52e9f84f82b3e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\regis\\.conda\\envs\\vit2\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\regis\\.conda\\envs\\vit2\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9dad2c78d84182ad576ff39e1f516f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\regis\\.conda\\envs\\vit2\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "# Determine the device type (CUDA if available, else CPU)\n",
    "# This ensures that the training leverages GPU acceleration if available for faster processing\n",
    "device_type = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initialize an early stopping callback\n",
    "# This will stop the training process if the validation loss does not improve after 3 epochs ('patience')\n",
    "# 'strict=False' allows training to continue for a few more steps even after early stopping condition is met\n",
    "# 'verbose=False' limits the amount of logging information during training\n",
    "# 'mode=min' indicates that training should stop when the monitored quantity (validation loss) stops decreasing\n",
    "early_stop_callback = EarlyStopping(monitor='validation_loss', patience=3, strict=False, verbose=False, mode='min')\n",
    "\n",
    "# Initialize the PyTorch Lightning Trainer\n",
    "# 'max_epochs=-1' indicates an indefinite number of epochs, but early stopping will intervene\n",
    "# 'accelerator=device_type' specifies the computation device (GPU or CPU)\n",
    "# The early stopping callback is added to the list of callbacks\n",
    "trainer = Trainer(max_epochs=-1, accelerator=device_type, callbacks=[early_stop_callback])\n",
    "\n",
    "# Start the training process\n",
    "# The model is trained using the specified train and validation data loaders\n",
    "# The training process will automatically use the device specified earlier and apply early stopping as configured\n",
    "trainer.fit(model, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the total runtime\n",
    "print(f'Total runtime: {np.round((time.time()-start_time) / 60, 2)} minutes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ViT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
